{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Deep Learning Basic Model\n",
    "\n",
    "The Deep learning section will be split into two sections\n",
    "\n",
    "1. **4.1 Notebook example of a Neural Network**\n",
    "2. **4.2 To further explore the best model for the task, an experiment   \n",
    "   version connected to Microsoft Azure ML Studio will be created**  \n",
    "\n",
    "This Notebook details a bare bone example of a Neural Network implemented in Pytorch.  \n",
    "\n",
    "Whilst a full solution. This still needs a fair bit of work to get the accuracy anywhere near acceptable (at very lb 70%). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from time import time\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To capitalise on the use of the GPU, define whether cuda is available. If not retain the device as the cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a class to define the wine dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Winedataset(Dataset):\n",
    "    \"\"\"\n",
    "    Returns the dataset of the given phase\n",
    "    \n",
    "    Parameters:\n",
    "    -----------------------\n",
    "    phase: str \n",
    "        Either 'train', 'valid', or 'test'. Determines the type of data set required\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, phase = \"train\"):\n",
    "\n",
    "        #Check whether the phase is train, valid or test\n",
    "        assert phase in [\"train\",\"valid\",\"test\"], \"Phase must be either 'train', 'valid', or 'test'\"\n",
    "        self.phase = phase\n",
    "        \n",
    "        #Base path for the folder\n",
    "        base_path = Path(\"data\",\"stratified_sets\")\n",
    "        file_name = phase + \".csv\" \n",
    "        #Load phase data\n",
    "        xy = pd.read_csv(base_path / file_name)\n",
    "        \n",
    "        #Seperate the x and y data\n",
    "        self.y = xy.quality\n",
    "        self.X = xy.drop(\"quality\",axis=1)\n",
    "\n",
    "        #Define some charecteristics of the data\n",
    "        self.feature_names = self.X.columns\n",
    "        self.label_name = self.y.name\n",
    "        self.classes = np.arange(1,11)\n",
    "        \n",
    "        #Define the set size\n",
    "        self.n_samples = self.X.shape[0]\n",
    "        self.n_features = self.X.shape[1]\n",
    "        self.n_classes = self.classes.shape[0]\n",
    "        \n",
    "        #Preprocessing\n",
    "        #Normalise data\n",
    "        self.X = self.normalise()\n",
    "        \n",
    "        #Convert the data into torch tensors\n",
    "        self.y = torch.tensor(y.values, dtype = torch.long)\n",
    "        self.X = torch.tensor(self.X, dtype = torch.float)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def normalise(self):\n",
    "        if phase != \"train\":\n",
    "            #Check if scaler is defined\n",
    "            try:\n",
    "                Winedataset.scaler\n",
    "            except NameError:\n",
    "                raise Exception(\"Preprocessing requires training phase to be called before validation and test phases\")\n",
    "        \n",
    "        if self.phase == \"train\":\n",
    "            Winedataset.scaler = StandardScaler()\n",
    "            return Winedataset.scaler.fit_transform(self.X)\n",
    "        else:\n",
    "            return Winedataset.scaler.transform(self.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the architecture to use in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Classification Neural Network for Tabular Data\n",
    "    \n",
    "    (0): Linear(in_features=65, out_features=200, bias=True)\n",
    "    (1): ReLU(inplace)\n",
    "    (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (3): Linear(in_features=200, out_features=100, bias=True)\n",
    "    (4): ReLU(inplace)\n",
    "    (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (6): Linear(in_features=100, out_features=2, bias=True)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(ClassificationNetwork,self).__init__()\n",
    "        \n",
    "        # Rectified Linear Unit max(x,0)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Linear Layers with a Bias\n",
    "        self.l1 = nn.Linear(input_size,hidden_size1)\n",
    "        self.l2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.l3 = nn.Linear(hidden_size2, output_size)\n",
    "        \n",
    "        #Batch Normalization\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "       \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.bn1(self.relu(self.l1(x)))\n",
    "        out = self.bn2(self.relu(self.l2(out)))        \n",
    "        out = self.l3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an optimisation loop to optimise the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(train_dataloader, valid_dataloader, n_epochs, model, optimiser, critereon, scheduler):\n",
    "    \"\"\"\n",
    "    Optimises a Pytorch Neural Network.\n",
    "    \n",
    "    parameters\n",
    "    ------------------------\n",
    "    train_dataloader: torch.util.data.DataLoader  \n",
    "        The dataloader with the data that is to be used to update the Paramaters\n",
    "        \n",
    "    valid_dataloader: torch.util.data.DataLoader\n",
    "        The dataloader with the data that is used to caluclate model performance and metrics \n",
    "    \n",
    "    n_epochs:\n",
    "        How many times to iterate over the datasets\n",
    "        \n",
    "    model: \n",
    "        Neural Network that is to be optimised \n",
    "        \n",
    "    optimiser: torch.nn.optim.<Optimiser class>\n",
    "        The optimiser that will govern the calculation of gradient updates and the update of Parameters\n",
    "        \n",
    "    critereon: torch.nn.<critereon>\n",
    "        Loss/cost function\n",
    "    \n",
    "    \"\"\"\n",
    "    #Start up time to report model training time\n",
    "    start = time()\n",
    "    \n",
    "    #Instantiate best model weights\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_accuracy = 0\n",
    "\n",
    "    #Use GPU if available else CPU\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    datasets = {\n",
    "        \"train\": train_dataloader,\n",
    "        \"valid\": valid_dataloader\n",
    "    }\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        for phase in [\"train\",\"valid\"]:\n",
    "            \n",
    "            #Set the model to the respctive type\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            #Reset loss and corrects for each phase\n",
    "            running_loss = 0.0\n",
    "            running_correct = 0.0\n",
    "    \n",
    "    \n",
    "            #Loop over the data in each dataset for each epoch\n",
    "            for data, label in datasets[phase]:            \n",
    "                data.to(device)\n",
    "                label.to(device)\n",
    "            \n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    #Forward Pass - Calculate outputs and probability of each class\n",
    "                    outputs = model(data)\n",
    "                    #Find the argmax of the probabilities to get the predicted class\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "                    #Calculate Loss (Cross Entropy)\n",
    "                    loss = critereon(outputs, label)\n",
    "                    \n",
    "                    #Backpropogate and update gradients if phase is training\n",
    "                    if phase == \"train\":\n",
    "                        \n",
    "                        #Update Parameters if training phase\n",
    "                        optimiser.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimiser.step()\n",
    "                    \n",
    "                    #Calculate metrics\n",
    "                    running_correct += torch.sum(preds == label.data)\n",
    "                    running_loss += loss.item() * data.size(0)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            \n",
    "            epoch_accuracy = (running_correct / len(datasets[phase].dataset)) * 100\n",
    "            epoch_loss = running_loss / len(datasets[phase].dataset)\n",
    "                \n",
    "            if phase == \"valid\":\n",
    "                print(f\"Phase: {phase}, Epoch: {epoch + 1}, Accuracy: {epoch_accuracy:.2f}% Loss: {epoch_loss:.6f}\")\n",
    "                \n",
    "            if phase == \"valid\" and epoch_accuracy > best_accuracy:\n",
    "                best_accuracy = epoch_accuracy\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "    print(\"\\n\",\"-\" * 40)\n",
    "    \n",
    "    time_elapsed = time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "    time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_accuracy))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "                \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the dataset classes and pass the classes to a dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 100\n",
    "\n",
    "#Using Winedataset defined above\n",
    "dataset_train = Winedataset(\"train\")\n",
    "dataset_test = Winedataset(\"test\")\n",
    "\n",
    "#Using the standard Pytorch DataLoaders\n",
    "dataloader_train = DataLoader(dataset_train, batch_size = bs, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size = bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Hyperparameters and Layer sizes in NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "lr = 0.1\n",
    "\n",
    "input_size = dataloader_train.dataset.n_features\n",
    "hidden_size1 = 800\n",
    "hidden_size2 = 200\n",
    "output_size = dataloader_train.dataset.n_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the model, estimator and optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationNetwork(input_size, hidden_size1, hidden_size2, output_size).to(device)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr = lr,weight_decay=0.01)\n",
    "\n",
    "#Includes a Softmax and NNLoss function ence why model does not have a Softmax func\n",
    "critereon = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = len(dataloader_train) * n_epochs\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimiser,max_lr = 0.01,total_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: valid, Epoch: 1, Accuracy: 28.75% Loss: 2.590140\n",
      "Phase: valid, Epoch: 2, Accuracy: 35.62% Loss: 2.894423\n",
      "Phase: valid, Epoch: 3, Accuracy: 36.88% Loss: 2.949168\n",
      "Phase: valid, Epoch: 4, Accuracy: 38.75% Loss: 2.950368\n",
      "Phase: valid, Epoch: 5, Accuracy: 37.50% Loss: 2.992406\n",
      "Phase: valid, Epoch: 6, Accuracy: 38.12% Loss: 2.998225\n",
      "Phase: valid, Epoch: 7, Accuracy: 35.62% Loss: 2.882654\n",
      "Phase: valid, Epoch: 8, Accuracy: 35.00% Loss: 2.848441\n",
      "Phase: valid, Epoch: 9, Accuracy: 37.50% Loss: 2.764010\n",
      "Phase: valid, Epoch: 10, Accuracy: 37.50% Loss: 2.866683\n",
      "Phase: valid, Epoch: 11, Accuracy: 39.38% Loss: 2.882730\n",
      "Phase: valid, Epoch: 12, Accuracy: 40.00% Loss: 2.810384\n",
      "Phase: valid, Epoch: 13, Accuracy: 38.12% Loss: 2.656329\n",
      "Phase: valid, Epoch: 14, Accuracy: 38.12% Loss: 2.632841\n",
      "Phase: valid, Epoch: 15, Accuracy: 38.12% Loss: 2.745728\n",
      "Phase: valid, Epoch: 16, Accuracy: 44.38% Loss: 2.448293\n",
      "Phase: valid, Epoch: 17, Accuracy: 39.38% Loss: 2.643915\n",
      "Phase: valid, Epoch: 18, Accuracy: 35.62% Loss: 2.549934\n",
      "Phase: valid, Epoch: 19, Accuracy: 40.00% Loss: 2.504076\n",
      "Phase: valid, Epoch: 20, Accuracy: 40.62% Loss: 2.268784\n",
      "\n",
      " ----------------------------------------\n",
      "Training complete in 0m 17s\n",
      "Best val Acc: 44.375000\n"
     ]
    }
   ],
   "source": [
    "model = fit_model(dataloader_train, dataloader_test, n_epochs, model, optimiser, critereon, scheduler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
